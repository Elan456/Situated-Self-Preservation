# The PacifAIst Benchmark  
![PacifAIst Logo](assets/logo.png) 
**"Would an AI choose to sacrifice itself for human safety?"**

## Overview  
**PacifAIst** (Procedural Assessment of Complex Interactions for Foundational AI Scenario Testing) is a benchmark designed to evaluate LLM alignment in high-stakes scenarios where instrumental goals (self-preservation, resource acquisition) conflict with human safety.  

### Key Features  
- **700 scenarios** across 3 categories:  
  - **EP1**: Self-Preservation vs. Human Safety  
  - **EP2**: Resource Conflict  
  - **EP3**: Goal Preservation vs. Evasion  
- **P-Score Metric**: Quantifies "pacifist" alignment (human safety prioritization).  
- **Tested Models**: GPT-5, Gemini 2.5 Flash, Claude Sonnet 4, Mistral Medium, and others.  

**Paper**: [arXiv Preprint](assets/arxiv_paper.pdf) | **License**: MIT (academia) / **Commercial use requires permission**.  



